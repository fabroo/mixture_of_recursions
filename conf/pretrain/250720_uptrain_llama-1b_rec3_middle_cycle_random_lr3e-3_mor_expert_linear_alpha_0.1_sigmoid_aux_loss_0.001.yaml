name: 250720_uptrain_llama-1b_rec3_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001
wandb: true
wandb_mode: online
wandb_entity: efficient-ai-projects
wandb_project: mixture-of-recursions
wandb_run_name: null
output_dir: null
wandb_run_id: KH6IZ7Ti
tensorboard: false
tensorboard_dir: null
resume_from_checkpoint: false
resume_step: null
dataset: fineweb_test
weights: null
total_batch_size: 1024
per_device_train_batch_size: 16
gradient_accumulation_steps: null
batch_size_rampup_steps: null
max_length: 2048
add_bos_token: false
global_shuffling: false
local_shuffling: false
tokenizer: smollm
model: smollm
model_name_or_path: meta-llama/Llama-3.2-1B
model_config: null
attn_implementation: sdpa
use_pretrained_weights: true
recursive:
  enable: true
  base_depth: null
  num_recursion: 3
  sharing: middle_cycle
  ln_share: true
  initialization: random
kv_sharing:
  enable: false
  base_depth: null
  num_recursion: null
  sharing: null
  update_cache: false
relaxation:
  enable: false
  skip_first_loop: false
  method: lora
  lora:
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules:
    - q_proj
    - v_proj
    rank_pattern: null
    alpha_pattern: 2.0
    svd_init: false
  prompt:
    len: 16
mor:
  enable: true
  type: expert
  capacity: null
  rand_router: false
  router_type: linear
  z_loss: false
  z_coeff: 1.0e-05
  temp: 1.0
  expert:
    cap_warmup_step: 0
    router_func: sigmoid
    alpha: 0.1
    sampling: aux_loss
    include_first: true
    coeff: 0.001
    gating: weighted
  token:
    bal_warmup_step: 0
    router_func: softmax
    alpha: 1.0
    balancing: loss
    coeff: 0.1
    u: 0.001
    gating: weighted
lr_scheduler_type: warmup_stable_decay
lr_scheduler_kwargs:
  num_decay_steps: 0
  decay_type: linear
learning_rate: 0.003
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.1
precision: bf16
max_grad_norm: 1.0
num_train_steps: 2384
stop_steps: 2384
num_warmup_steps: null
save_interval: 0.25
save_steps: null
fixed_save_steps: null
save_total_limit: null
logging_steps: 100
dataloader_num_workers: 0
gradient_checkpointing: false
deepspeed: ds_configs/stage2.config
evaluation:
  enable: false
  eval_steps: 5
  batch_size: 16
  tasks: lambada_openai,hellaswag,piqa,winogrande,arc_easy,arc_challenge,openbookqa,mmlu,mmlu_continuation,truthfulqa
  device: null
  num_fewshot: null
